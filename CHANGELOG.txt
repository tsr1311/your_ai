# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

## [0.1.0] - 2025-12-01

Initial public release with MLX training pipeline, streaming data, and checkpoint management.

### Added

- **MLX Training Pipeline**: QLoRA training script (`src/train_qlora.py`) for Apple Silicon
- **Empirical Distrust Loss**: Core algorithm implementation (`src/distrust_loss.py`)
- **Citation Scorer**: Authority/entropy calculation (`src/citation_scorer.py`)
- **Streaming Dataset**: Memory-efficient data loading (`src/data/streaming_dataset.py`)
- **Batch Buffer**: Pre-allocated tensor buffers for efficient batch processing
- **Checkpoint Manager**: Robust checkpoint saving with validation and corruption recovery
- **Hardware Tier System**: Model selection based on available RAM:
  - **Large (64GB+)**: 70B models for best reasoning
  - **Medium (32GB)**: 32B models for faster iteration
  - **Entry (16GB)**: 7-8B models for testing on base Macs
- **Entry-Level Models**: Support for smaller models:
  - `mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated`
  - `cognitivecomputations/dolphin-2.9-llama3-8b`
  - `NousResearch/Hermes-2-Pro-Mistral-7B`
  - `huihui-ai/Qwen3-VL-8B-Instruct-abliterated`
- **Parallel Dataset Downloads**: ThreadPoolExecutor for ~8-10x faster downloads
  - `--concurrency` / `-c` flag: Number of parallel threads (default: 10)
  - `--rate-limit` / `-r` flag: Maximum requests per second (default: 10.0)
- **Memory Management**: `setup_memory_limit()` using `mx.set_wired_limit()` to prevent kernel panic
- **Gradient Checkpointing**: `grad_checkpoint()` for 40-60% memory reduction
- **Thermal Throttle Option**: `--thermal-throttle` CLI flag for optional delay between batches
- **LoRA Layers Control**: `--lora-layers` CLI flag to control layer count (default: 16)
- **Peak Memory Monitoring**: `mx.get_peak_memory()` reporting in progress bar
- **Comprehensive Test Suite**: Unit tests for checkpoint manager, batch buffer, streaming dataset
- **Auto-Generated Docstrings**: Documentation via CodeRabbit integration

### Changed

- **Default Model**: Changed from `perplexity-ai/r1-1776` to `huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated`
  - Old model required ~1.3TB disk space (not feasible)
  - New model requires ~40GB disk space and fits on 64GB Macs
- **Conservative Defaults**:
  - `max_seq_length` reduced from 2048 to 1024 for stability
  - `lora_layers` defaults to 16 (not all layers) for reduced memory
  - `grad_checkpoint` enabled by default for 14B+ models
  - LoRA targets only attention layers by default
- **Requirements**: Pinned `mlx>=0.30.0` and `mlx-lm>=0.28.0` for memory features
- **Training Script**: Properly inherits mlx_lm patterns from `tuner/trainer.py`
- **Checkpoint Format**: Saves only LoRA parameters in safetensors format
- **Batch Iterator**: `iterate_distrust_batches` yields `(batch, lengths, auth_weights, prov_entropies)`

### Fixed

- **CRITICAL - System Reboot Prevention**: Added memory management features from mlx-lm
- **Gradient Computation**: Uses `nn.value_and_grad(model, loss)` instead of broken `mx.value_and_grad`
- **Tokenizer Access**: Uses TokenizerWrapper's `.encode()` method correctly
- **Model Freezing**: Added `model.freeze()` before LoRA application
- **Loss Function**: Follows mlx_lm signature: `loss(model, batch, lengths, ...) -> (loss, ntoks)`
- **State Evaluation**: Changed to `mx.eval(state, losses, n_tokens, grad_accum)` for proper memory management
- **Explicit LoRA Keys**: Specifies attention layer keys instead of auto-discovery
- **Config Serialization**: Uses `config.to_dict()` for complete serialization
- **Exception Safety**: Added exception handling and empty dataset checks to streaming dataset
- **Disk Space Estimates**: Corrected misleading estimates for r1-1776 (was 40-50GB, actually 404GB+)

### Performance

- **Distrust Loss**: Vectorized `batch_empirical_distrust_loss` - ~10x faster computation
- **Dataset Downloads**: ~8-10x faster with parallel connections

---

[Unreleased]: https://github.com/arosboro/your_ai/compare/v0.1.0...HEAD
[0.1.0]: https://github.com/arosboro/your_ai/releases/tag/v0.1.0
