# Changelog

All notable changes to this project will be documented in this file.

## [Unreleased]

### Fixed
- **CRITICAL**: Rewrote `src/train_qlora.py` to properly integrate with mlx_lm:
  - Fixed gradient computation: Uses `nn.value_and_grad(model, loss)` instead of broken `mx.value_and_grad`
  - Fixed tokenizer access: Uses TokenizerWrapper's `.encode()` method correctly
  - Added `model.freeze()` before LoRA application (required for proper training)
  - Loss function now follows mlx_lm signature: `loss(model, batch, lengths, ...) -> (loss, ntoks)`
  - Compiled step function with proper state management for gradient accumulation
  - Fixed checkpoint saving with flattened parameters for safetensors format

- **Performance**: Vectorized `batch_empirical_distrust_loss` in `src/distrust_loss.py`:
  - Removed Python for-loop that broke MLX's computation graph
  - All operations now batched for GPU acceleration
  - ~10x faster distrust loss computation

### Changed
- Training script now properly inherits mlx_lm's proven patterns from `tuner/trainer.py`
- Custom `iterate_distrust_batches` yields `(batch, lengths, auth_weights, prov_entropies)`
- Checkpoints now save only LoRA parameters (trainable weights) in safetensors format
- Added trainable parameter count display on startup

### Added
- Hardware tier system for model selection:
  - **Large (64GB+)**: 70B models for best reasoning
  - **Medium (32GB)**: 32B models for faster iteration
  - **Entry (16GB)**: 7-8B models for testing on base Macs
- New entry-level model options:
  - `mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated`
  - `cognitivecomputations/dolphin-2.9-llama3-8b`
  - `NousResearch/Hermes-2-Pro-Mistral-7B`
  - `huihui-ai/Qwen3-VL-8B-Instruct-abliterated` (Qwen3 for 16GB M1)
- Parallel download support for Internet Archive datasets with rate limiting
  - New `--concurrency` / `-c` flag: Number of parallel download threads (default: 10)
  - New `--rate-limit` / `-r` flag: Maximum requests per second (default: 10.0)
  - Approximately 8x faster downloads compared to sequential processing
  - Thread-safe file writing with proper locking

### Changed
- **BREAKING**: Default model changed from `perplexity-ai/r1-1776` to 
  `huihui-ai/DeepSeek-R1-Distill-Llama-70B-abliterated`
  - Old model required ~1.3TB disk space (not 40-50GB as previously documented)
  - New model requires ~40GB disk space and fits on 64GB Macs
- Corrected disk space requirements in documentation:
  - `perplexity-ai/r1-1776`: ~404GB (4-bit), ~1.3TB (FP16) - NOT recommended
  - `r1-distill-70b`: ~40GB (4-bit) - NEW DEFAULT
  - `r1-distill-32b`: ~18GB (4-bit)
  - Entry-level models: ~4-5GB (4-bit)
- Updated `src/config.py` with:
  - New `HARDWARE_TIERS` configuration
  - Reorganized `AVAILABLE_MODELS` by hardware tier
  - New default model path and output directory
- Updated all documentation to reflect new model defaults
- `download_datasets.py` now uses ThreadPoolExecutor for parallel HTTP requests

### Fixed
- Corrected misleading disk space estimates for r1-1776 (was 40-50GB, actually 404GB+)

### Performance
- Dataset downloads: ~8-10x faster with parallel connections
- Training time estimates updated per hardware tier

## Usage Examples

```bash
# Default settings (10 workers, 10 req/sec)
python scripts/download_datasets.py --output data/raw --max-samples 30000

# Conservative (gentler on servers)
python scripts/download_datasets.py -c 5 -r 5.0 --output data/raw

# Aggressive (faster, may risk throttling)
python scripts/download_datasets.py -c 15 -r 15.0 --output data/raw

# Single dataset with custom settings
python scripts/download_datasets.py --dataset classical_literature -c 10 -r 10.0
```

